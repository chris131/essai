{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    },
    "colab": {
      "name": "train_shapes.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFG4vMqqyiLx",
        "colab_type": "text"
      },
      "source": [
        "# Mask R-CNN - Train on Shapes Dataset\n",
        "\n",
        "\n",
        "This notebook shows how to train Mask R-CNN on your own dataset. To keep things simple we use a synthetic dataset of shapes (squares, triangles, and circles) which enables fast training. You'd still need a GPU, though, because the network backbone is a Resnet101, which would be too slow to train on a CPU. On a GPU, you can start to get okay-ish results in a few minutes, and good results in less than an hour.\n",
        "\n",
        "The code of the *Shapes* dataset is included below. It generates images on the fly, so it doesn't require downloading any data. And it can generate images of any size, so we pick a small image size to train faster. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oo-JcQLZyrrb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "7cb88258-8c91-4f83-db5e-e4f25b8cb9a6"
      },
      "source": [
        "!git clone https://github.com/matterport/Mask_RCNN.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Mask_RCNN'...\n",
            "remote: Enumerating objects: 956, done.\u001b[K\n",
            "remote: Total 956 (delta 0), reused 0 (delta 0), pack-reused 956\u001b[K\n",
            "Receiving objects: 100% (956/956), 111.83 MiB | 39.03 MiB/s, done.\n",
            "Resolving deltas: 100% (570/570), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xR6H2TzTyqb_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e1563eeb-4ece-49f2-e21b-7f5232b105e6"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "2.1.0-rc1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3o4YAENB1yUr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        },
        "outputId": "6fef79da-a2ac-44c4-91c4-916eeb805abf"
      },
      "source": [
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdGw33uwyr26",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('Mask_RCNN')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqIck7u6yiL0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4c5f7aae-10e5-4e65-ede0-317c5b5d0b42"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import random\n",
        "import math\n",
        "import re\n",
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Root directory of the project\n",
        "ROOT_DIR = os.path.abspath(\"../../\")\n",
        "\n",
        "# Import Mask RCNN\n",
        "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
        "from mrcnn.config import Config\n",
        "from mrcnn import utils\n",
        "import mrcnn.model as modellib\n",
        "from mrcnn import visualize\n",
        "from mrcnn.model import log\n",
        "\n",
        "%matplotlib inline \n",
        "\n",
        "# Directory to save logs and trained model\n",
        "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
        "\n",
        "# Local path to trained weights file\n",
        "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
        "# Download COCO trained weights from Releases if needed\n",
        "if not os.path.exists(COCO_MODEL_PATH):\n",
        "    utils.download_trained_weights(COCO_MODEL_PATH)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XdyDjcMyiL5",
        "colab_type": "text"
      },
      "source": [
        "## Configurations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSC5MzkuyiL5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        },
        "outputId": "00421534-b10e-4c62-c609-7994ead20a1f"
      },
      "source": [
        "class ShapesConfig(Config):\n",
        "    \"\"\"Configuration for training on the toy shapes dataset.\n",
        "    Derives from the base Config class and overrides values specific\n",
        "    to the toy shapes dataset.\n",
        "    \"\"\"\n",
        "    # Give the configuration a recognizable name\n",
        "    NAME = \"shapes\"\n",
        "\n",
        "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
        "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 8\n",
        "\n",
        "    # Number of classes (including background)\n",
        "    NUM_CLASSES = 1 + 3  # background + 3 shapes\n",
        "\n",
        "    # Use small images for faster training. Set the limits of the small side\n",
        "    # the large side, and that determines the image shape.\n",
        "    IMAGE_MIN_DIM = 128\n",
        "    IMAGE_MAX_DIM = 128\n",
        "\n",
        "    # Use smaller anchors because our image and objects are small\n",
        "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n",
        "\n",
        "    # Reduce training ROIs per image because the images are small and have\n",
        "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
        "    TRAIN_ROIS_PER_IMAGE = 32\n",
        "\n",
        "    # Use a small epoch since the data is simple\n",
        "    STEPS_PER_EPOCH = 100\n",
        "\n",
        "    # use small validation steps since the epoch is small\n",
        "    VALIDATION_STEPS = 5\n",
        "    \n",
        "config = ShapesConfig()\n",
        "config.display()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Configurations:\n",
            "BACKBONE                       resnet101\n",
            "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
            "BATCH_SIZE                     8\n",
            "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
            "COMPUTE_BACKBONE_SHAPE         None\n",
            "DETECTION_MAX_INSTANCES        100\n",
            "DETECTION_MIN_CONFIDENCE       0.7\n",
            "DETECTION_NMS_THRESHOLD        0.3\n",
            "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
            "GPU_COUNT                      1\n",
            "GRADIENT_CLIP_NORM             5.0\n",
            "IMAGES_PER_GPU                 8\n",
            "IMAGE_CHANNEL_COUNT            3\n",
            "IMAGE_MAX_DIM                  128\n",
            "IMAGE_META_SIZE                16\n",
            "IMAGE_MIN_DIM                  128\n",
            "IMAGE_MIN_SCALE                0\n",
            "IMAGE_RESIZE_MODE              square\n",
            "IMAGE_SHAPE                    [128 128   3]\n",
            "LEARNING_MOMENTUM              0.9\n",
            "LEARNING_RATE                  0.001\n",
            "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
            "MASK_POOL_SIZE                 14\n",
            "MASK_SHAPE                     [28, 28]\n",
            "MAX_GT_INSTANCES               100\n",
            "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
            "MINI_MASK_SHAPE                (56, 56)\n",
            "NAME                           shapes\n",
            "NUM_CLASSES                    4\n",
            "POOL_SIZE                      7\n",
            "POST_NMS_ROIS_INFERENCE        1000\n",
            "POST_NMS_ROIS_TRAINING         2000\n",
            "PRE_NMS_LIMIT                  6000\n",
            "ROI_POSITIVE_RATIO             0.33\n",
            "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
            "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
            "RPN_ANCHOR_STRIDE              1\n",
            "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
            "RPN_NMS_THRESHOLD              0.7\n",
            "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
            "STEPS_PER_EPOCH                100\n",
            "TOP_DOWN_PYRAMID_SIZE          256\n",
            "TRAIN_BN                       False\n",
            "TRAIN_ROIS_PER_IMAGE           32\n",
            "USE_MINI_MASK                  True\n",
            "USE_RPN_ROIS                   True\n",
            "VALIDATION_STEPS               5\n",
            "WEIGHT_DECAY                   0.0001\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRUiyq8pyiL7",
        "colab_type": "text"
      },
      "source": [
        "## Notebook Preferences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrdqGthlyiL8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_ax(rows=1, cols=1, size=8):\n",
        "    \"\"\"Return a Matplotlib Axes array to be used in\n",
        "    all visualizations in the notebook. Provide a\n",
        "    central point to control graph sizes.\n",
        "    \n",
        "    Change the default size attribute to control the size\n",
        "    of rendered images\n",
        "    \"\"\"\n",
        "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
        "    return ax"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwkDRAHjyiL9",
        "colab_type": "text"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "Create a synthetic dataset\n",
        "\n",
        "Extend the Dataset class and add a method to load the shapes dataset, `load_shapes()`, and override the following methods:\n",
        "\n",
        "* load_image()\n",
        "* load_mask()\n",
        "* image_reference()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgzI0hnZyiL-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ShapesDataset(utils.Dataset):\n",
        "    \"\"\"Generates the shapes synthetic dataset. The dataset consists of simple\n",
        "    shapes (triangles, squares, circles) placed randomly on a blank surface.\n",
        "    The images are generated on the fly. No file access required.\n",
        "    \"\"\"\n",
        "\n",
        "    def load_shapes(self, count, height, width):\n",
        "        \"\"\"Generate the requested number of synthetic images.\n",
        "        count: number of images to generate.\n",
        "        height, width: the size of the generated images.\n",
        "        \"\"\"\n",
        "        # Add classes\n",
        "        self.add_class(\"shapes\", 1, \"square\")\n",
        "        self.add_class(\"shapes\", 2, \"circle\")\n",
        "        self.add_class(\"shapes\", 3, \"triangle\")\n",
        "\n",
        "        # Add images\n",
        "        # Generate random specifications of images (i.e. color and\n",
        "        # list of shapes sizes and locations). This is more compact than\n",
        "        # actual images. Images are generated on the fly in load_image().\n",
        "        for i in range(count):\n",
        "            bg_color, shapes = self.random_image(height, width)\n",
        "            self.add_image(\"shapes\", image_id=i, path=None,\n",
        "                           width=width, height=height,\n",
        "                           bg_color=bg_color, shapes=shapes)\n",
        "\n",
        "    def load_image(self, image_id):\n",
        "        \"\"\"Generate an image from the specs of the given image ID.\n",
        "        Typically this function loads the image from a file, but\n",
        "        in this case it generates the image on the fly from the\n",
        "        specs in image_info.\n",
        "        \"\"\"\n",
        "        info = self.image_info[image_id]\n",
        "        bg_color = np.array(info['bg_color']).reshape([1, 1, 3])\n",
        "        image = np.ones([info['height'], info['width'], 3], dtype=np.uint8)\n",
        "        image = image * bg_color.astype(np.uint8)\n",
        "        for shape, color, dims in info['shapes']:\n",
        "            image = self.draw_shape(image, shape, dims, color)\n",
        "        return image\n",
        "\n",
        "    def image_reference(self, image_id):\n",
        "        \"\"\"Return the shapes data of the image.\"\"\"\n",
        "        info = self.image_info[image_id]\n",
        "        if info[\"source\"] == \"shapes\":\n",
        "            return info[\"shapes\"]\n",
        "        else:\n",
        "            super(self.__class__).image_reference(self, image_id)\n",
        "\n",
        "    def load_mask(self, image_id):\n",
        "        \"\"\"Generate instance masks for shapes of the given image ID.\n",
        "        \"\"\"\n",
        "        info = self.image_info[image_id]\n",
        "        shapes = info['shapes']\n",
        "        count = len(shapes)\n",
        "        mask = np.zeros([info['height'], info['width'], count], dtype=np.uint8)\n",
        "        for i, (shape, _, dims) in enumerate(info['shapes']):\n",
        "            mask[:, :, i:i+1] = self.draw_shape(mask[:, :, i:i+1].copy(),\n",
        "                                                shape, dims, 1)\n",
        "        # Handle occlusions\n",
        "        occlusion = np.logical_not(mask[:, :, -1]).astype(np.uint8)\n",
        "        for i in range(count-2, -1, -1):\n",
        "            mask[:, :, i] = mask[:, :, i] * occlusion\n",
        "            occlusion = np.logical_and(occlusion, np.logical_not(mask[:, :, i]))\n",
        "        # Map class names to class IDs.\n",
        "        class_ids = np.array([self.class_names.index(s[0]) for s in shapes])\n",
        "        return mask.astype(np.bool), class_ids.astype(np.int32)\n",
        "\n",
        "    def draw_shape(self, image, shape, dims, color):\n",
        "        \"\"\"Draws a shape from the given specs.\"\"\"\n",
        "        # Get the center x, y and the size s\n",
        "        x, y, s = dims\n",
        "        if shape == 'square':\n",
        "            cv2.rectangle(image, (x-s, y-s), (x+s, y+s), color, -1)\n",
        "        elif shape == \"circle\":\n",
        "            cv2.circle(image, (x, y), s, color, -1)\n",
        "        elif shape == \"triangle\":\n",
        "            points = np.array([[(x, y-s),\n",
        "                                (x-s/math.sin(math.radians(60)), y+s),\n",
        "                                (x+s/math.sin(math.radians(60)), y+s),\n",
        "                                ]], dtype=np.int32)\n",
        "            cv2.fillPoly(image, points, color)\n",
        "        return image\n",
        "\n",
        "    def random_shape(self, height, width):\n",
        "        \"\"\"Generates specifications of a random shape that lies within\n",
        "        the given height and width boundaries.\n",
        "        Returns a tuple of three valus:\n",
        "        * The shape name (square, circle, ...)\n",
        "        * Shape color: a tuple of 3 values, RGB.\n",
        "        * Shape dimensions: A tuple of values that define the shape size\n",
        "                            and location. Differs per shape type.\n",
        "        \"\"\"\n",
        "        # Shape\n",
        "        shape = random.choice([\"square\", \"circle\", \"triangle\"])\n",
        "        # Color\n",
        "        color = tuple([random.randint(0, 255) for _ in range(3)])\n",
        "        # Center x, y\n",
        "        buffer = 20\n",
        "        y = random.randint(buffer, height - buffer - 1)\n",
        "        x = random.randint(buffer, width - buffer - 1)\n",
        "        # Size\n",
        "        s = random.randint(buffer, height//4)\n",
        "        return shape, color, (x, y, s)\n",
        "\n",
        "    def random_image(self, height, width):\n",
        "        \"\"\"Creates random specifications of an image with multiple shapes.\n",
        "        Returns the background color of the image and a list of shape\n",
        "        specifications that can be used to draw the image.\n",
        "        \"\"\"\n",
        "        # Pick random background color\n",
        "        bg_color = np.array([random.randint(0, 255) for _ in range(3)])\n",
        "        # Generate a few random shapes and record their\n",
        "        # bounding boxes\n",
        "        shapes = []\n",
        "        boxes = []\n",
        "        N = random.randint(1, 4)\n",
        "        for _ in range(N):\n",
        "            shape, color, dims = self.random_shape(height, width)\n",
        "            shapes.append((shape, color, dims))\n",
        "            x, y, s = dims\n",
        "            boxes.append([y-s, x-s, y+s, x+s])\n",
        "        # Apply non-max suppression wit 0.3 threshold to avoid\n",
        "        # shapes covering each other\n",
        "        keep_ixs = utils.non_max_suppression(np.array(boxes), np.arange(N), 0.3)\n",
        "        shapes = [s for i, s in enumerate(shapes) if i in keep_ixs]\n",
        "        return bg_color, shapes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yj4rK1e2yiMA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training dataset\n",
        "dataset_train = ShapesDataset()\n",
        "dataset_train.load_shapes(500, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
        "dataset_train.prepare()\n",
        "\n",
        "# Validation dataset\n",
        "dataset_val = ShapesDataset()\n",
        "dataset_val.load_shapes(50, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
        "dataset_val.prepare()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bB9BgMTNyiMB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "outputId": "1692d768-c69b-4969-dbc0-d633df1bae58"
      },
      "source": [
        "# Load and display random samples\n",
        "image_ids = np.random.choice(dataset_train.image_ids, 4)\n",
        "for image_id in image_ids:\n",
        "    image = dataset_train.load_image(image_id)\n",
        "    mask, class_ids = dataset_train.load_mask(image_id)\n",
        "    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxAAAACWCAYAAABO+G6lAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 1008x360 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxAAAACWCAYAAABO+G6lAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 1008x360 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxAAAACWCAYAAABO+G6lAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1008x360 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxAAAACWCAYAAABO+G6lAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAIDe/g9TAOIA56J0AgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1008x360 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRO2UhHWyiMD",
        "colab_type": "text"
      },
      "source": [
        "## Create Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1E30BhqkzssT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "635c79bc-f2a3-48cf-8a21-b397cea5dd50"
      },
      "source": [
        "os.getcwd()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xE6CnsC01qD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('mrcnn')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwYpNH4s1YVM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1d4c441e-a9f3-4ff8-a6b5-d24d24a2e7f5"
      },
      "source": [
        "# Directory to save logs and trained model\n",
        "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
        "print(MODEL_DIR)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/logs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1D64-QBuyiME",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "9e467835-b692-4758-941d-cc2d07d6fde1"
      },
      "source": [
        "# Create model in training mode\n",
        "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
        "                          model_dir=MODEL_DIR)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2139: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2239: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/Mask_RCNN/mrcnn/model.py:553: The name tf.random_shuffle is deprecated. Please use tf.random.shuffle instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/Mask_RCNN/mrcnn/utils.py:202: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/Mask_RCNN/mrcnn/model.py:600: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "box_ind is deprecated, use box_indices instead\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "ttj5rCzMyiMF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "7a372ac8-c429-4290-8254-cddea28fc673"
      },
      "source": [
        "# Which weights to start with?\n",
        "init_with = \"coco\"  # imagenet, coco, or last\n",
        "\n",
        "if init_with == \"imagenet\":\n",
        "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
        "elif init_with == \"coco\":\n",
        "    # Load weights trained on MS COCO, but skip layers that\n",
        "    # are different due to the different number of classes\n",
        "    # See README for instructions to download the COCO weights\n",
        "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
        "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
        "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
        "elif init_with == \"last\":\n",
        "    # Load the last model you trained and continue training\n",
        "    model.load_weights(model.find_last(), by_name=True)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhnGESh9yiMH",
        "colab_type": "text"
      },
      "source": [
        "## Training\n",
        "\n",
        "Train in two stages:\n",
        "1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function.\n",
        "\n",
        "2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers=\"all` to train all layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "d9PGPrRIyiMH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        },
        "outputId": "f7de49ec-c09d-4a36-c762-f1f41474abb3"
      },
      "source": [
        "# Train the head branches\n",
        "# Passing layers=\"heads\" freezes all layers except the head\n",
        "# layers. You can also pass a regular expression to select\n",
        "# which layers to train by name pattern.\n",
        "model.train(dataset_train, dataset_val, \n",
        "            learning_rate=config.LEARNING_RATE, \n",
        "            epochs=1, \n",
        "            layers='heads')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Starting at epoch 0. LR=0.001\n",
            "\n",
            "Checkpoint Path: /logs/shapes20191222T1342/mask_rcnn_shapes_{epoch:04d}.h5\n",
            "Selecting layers to train\n",
            "fpn_c5p5               (Conv2D)\n",
            "fpn_c4p4               (Conv2D)\n",
            "fpn_c3p3               (Conv2D)\n",
            "fpn_c2p2               (Conv2D)\n",
            "fpn_p5                 (Conv2D)\n",
            "fpn_p2                 (Conv2D)\n",
            "fpn_p3                 (Conv2D)\n",
            "fpn_p4                 (Conv2D)\n",
            "In model:  rpn_model\n",
            "    rpn_conv_shared        (Conv2D)\n",
            "    rpn_class_raw          (Conv2D)\n",
            "    rpn_bbox_pred          (Conv2D)\n",
            "mrcnn_mask_conv1       (TimeDistributed)\n",
            "mrcnn_mask_bn1         (TimeDistributed)\n",
            "mrcnn_mask_conv2       (TimeDistributed)\n",
            "mrcnn_mask_bn2         (TimeDistributed)\n",
            "mrcnn_class_conv1      (TimeDistributed)\n",
            "mrcnn_class_bn1        (TimeDistributed)\n",
            "mrcnn_mask_conv3       (TimeDistributed)\n",
            "mrcnn_mask_bn3         (TimeDistributed)\n",
            "mrcnn_class_conv2      (TimeDistributed)\n",
            "mrcnn_class_bn2        (TimeDistributed)\n",
            "mrcnn_mask_conv4       (TimeDistributed)\n",
            "mrcnn_mask_bn4         (TimeDistributed)\n",
            "mrcnn_bbox_fc          (TimeDistributed)\n",
            "mrcnn_mask_deconv      (TimeDistributed)\n",
            "mrcnn_class_logits     (TimeDistributed)\n",
            "mrcnn_mask             (TimeDistributed)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py:49: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence class.\n",
            "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1122: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1125: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "Epoch 1/1\n",
            "100/100 [==============================] - 3438s 34s/step - loss: 1.8544 - rpn_class_loss: 0.0313 - rpn_bbox_loss: 0.6338 - mrcnn_class_loss: 0.3209 - mrcnn_bbox_loss: 0.4277 - mrcnn_mask_loss: 0.4406 - val_loss: 1.0587 - val_rpn_class_loss: 0.0196 - val_rpn_bbox_loss: 0.4646 - val_mrcnn_class_loss: 0.1485 - val_mrcnn_bbox_loss: 0.1954 - val_mrcnn_mask_loss: 0.2307\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1265: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "44EhiiG6yiMJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0cb64d77-1257-4e59-d426-66adae918396"
      },
      "source": [
        "# Fine tune all layers\n",
        "# Passing layers=\"all\" trains all layers. You can also \n",
        "# pass a regular expression to select which layers to\n",
        "# train by name pattern.\n",
        "model.train(dataset_train, dataset_val, \n",
        "            learning_rate=config.LEARNING_RATE / 10,\n",
        "            epochs=2, \n",
        "            layers=\"all\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Starting at epoch 1. LR=0.0001\n",
            "\n",
            "Checkpoint Path: /logs/shapes20191222T1342/mask_rcnn_shapes_{epoch:04d}.h5\n",
            "Selecting layers to train\n",
            "conv1                  (Conv2D)\n",
            "bn_conv1               (BatchNorm)\n",
            "res2a_branch2a         (Conv2D)\n",
            "bn2a_branch2a          (BatchNorm)\n",
            "res2a_branch2b         (Conv2D)\n",
            "bn2a_branch2b          (BatchNorm)\n",
            "res2a_branch2c         (Conv2D)\n",
            "res2a_branch1          (Conv2D)\n",
            "bn2a_branch2c          (BatchNorm)\n",
            "bn2a_branch1           (BatchNorm)\n",
            "res2b_branch2a         (Conv2D)\n",
            "bn2b_branch2a          (BatchNorm)\n",
            "res2b_branch2b         (Conv2D)\n",
            "bn2b_branch2b          (BatchNorm)\n",
            "res2b_branch2c         (Conv2D)\n",
            "bn2b_branch2c          (BatchNorm)\n",
            "res2c_branch2a         (Conv2D)\n",
            "bn2c_branch2a          (BatchNorm)\n",
            "res2c_branch2b         (Conv2D)\n",
            "bn2c_branch2b          (BatchNorm)\n",
            "res2c_branch2c         (Conv2D)\n",
            "bn2c_branch2c          (BatchNorm)\n",
            "res3a_branch2a         (Conv2D)\n",
            "bn3a_branch2a          (BatchNorm)\n",
            "res3a_branch2b         (Conv2D)\n",
            "bn3a_branch2b          (BatchNorm)\n",
            "res3a_branch2c         (Conv2D)\n",
            "res3a_branch1          (Conv2D)\n",
            "bn3a_branch2c          (BatchNorm)\n",
            "bn3a_branch1           (BatchNorm)\n",
            "res3b_branch2a         (Conv2D)\n",
            "bn3b_branch2a          (BatchNorm)\n",
            "res3b_branch2b         (Conv2D)\n",
            "bn3b_branch2b          (BatchNorm)\n",
            "res3b_branch2c         (Conv2D)\n",
            "bn3b_branch2c          (BatchNorm)\n",
            "res3c_branch2a         (Conv2D)\n",
            "bn3c_branch2a          (BatchNorm)\n",
            "res3c_branch2b         (Conv2D)\n",
            "bn3c_branch2b          (BatchNorm)\n",
            "res3c_branch2c         (Conv2D)\n",
            "bn3c_branch2c          (BatchNorm)\n",
            "res3d_branch2a         (Conv2D)\n",
            "bn3d_branch2a          (BatchNorm)\n",
            "res3d_branch2b         (Conv2D)\n",
            "bn3d_branch2b          (BatchNorm)\n",
            "res3d_branch2c         (Conv2D)\n",
            "bn3d_branch2c          (BatchNorm)\n",
            "res4a_branch2a         (Conv2D)\n",
            "bn4a_branch2a          (BatchNorm)\n",
            "res4a_branch2b         (Conv2D)\n",
            "bn4a_branch2b          (BatchNorm)\n",
            "res4a_branch2c         (Conv2D)\n",
            "res4a_branch1          (Conv2D)\n",
            "bn4a_branch2c          (BatchNorm)\n",
            "bn4a_branch1           (BatchNorm)\n",
            "res4b_branch2a         (Conv2D)\n",
            "bn4b_branch2a          (BatchNorm)\n",
            "res4b_branch2b         (Conv2D)\n",
            "bn4b_branch2b          (BatchNorm)\n",
            "res4b_branch2c         (Conv2D)\n",
            "bn4b_branch2c          (BatchNorm)\n",
            "res4c_branch2a         (Conv2D)\n",
            "bn4c_branch2a          (BatchNorm)\n",
            "res4c_branch2b         (Conv2D)\n",
            "bn4c_branch2b          (BatchNorm)\n",
            "res4c_branch2c         (Conv2D)\n",
            "bn4c_branch2c          (BatchNorm)\n",
            "res4d_branch2a         (Conv2D)\n",
            "bn4d_branch2a          (BatchNorm)\n",
            "res4d_branch2b         (Conv2D)\n",
            "bn4d_branch2b          (BatchNorm)\n",
            "res4d_branch2c         (Conv2D)\n",
            "bn4d_branch2c          (BatchNorm)\n",
            "res4e_branch2a         (Conv2D)\n",
            "bn4e_branch2a          (BatchNorm)\n",
            "res4e_branch2b         (Conv2D)\n",
            "bn4e_branch2b          (BatchNorm)\n",
            "res4e_branch2c         (Conv2D)\n",
            "bn4e_branch2c          (BatchNorm)\n",
            "res4f_branch2a         (Conv2D)\n",
            "bn4f_branch2a          (BatchNorm)\n",
            "res4f_branch2b         (Conv2D)\n",
            "bn4f_branch2b          (BatchNorm)\n",
            "res4f_branch2c         (Conv2D)\n",
            "bn4f_branch2c          (BatchNorm)\n",
            "res4g_branch2a         (Conv2D)\n",
            "bn4g_branch2a          (BatchNorm)\n",
            "res4g_branch2b         (Conv2D)\n",
            "bn4g_branch2b          (BatchNorm)\n",
            "res4g_branch2c         (Conv2D)\n",
            "bn4g_branch2c          (BatchNorm)\n",
            "res4h_branch2a         (Conv2D)\n",
            "bn4h_branch2a          (BatchNorm)\n",
            "res4h_branch2b         (Conv2D)\n",
            "bn4h_branch2b          (BatchNorm)\n",
            "res4h_branch2c         (Conv2D)\n",
            "bn4h_branch2c          (BatchNorm)\n",
            "res4i_branch2a         (Conv2D)\n",
            "bn4i_branch2a          (BatchNorm)\n",
            "res4i_branch2b         (Conv2D)\n",
            "bn4i_branch2b          (BatchNorm)\n",
            "res4i_branch2c         (Conv2D)\n",
            "bn4i_branch2c          (BatchNorm)\n",
            "res4j_branch2a         (Conv2D)\n",
            "bn4j_branch2a          (BatchNorm)\n",
            "res4j_branch2b         (Conv2D)\n",
            "bn4j_branch2b          (BatchNorm)\n",
            "res4j_branch2c         (Conv2D)\n",
            "bn4j_branch2c          (BatchNorm)\n",
            "res4k_branch2a         (Conv2D)\n",
            "bn4k_branch2a          (BatchNorm)\n",
            "res4k_branch2b         (Conv2D)\n",
            "bn4k_branch2b          (BatchNorm)\n",
            "res4k_branch2c         (Conv2D)\n",
            "bn4k_branch2c          (BatchNorm)\n",
            "res4l_branch2a         (Conv2D)\n",
            "bn4l_branch2a          (BatchNorm)\n",
            "res4l_branch2b         (Conv2D)\n",
            "bn4l_branch2b          (BatchNorm)\n",
            "res4l_branch2c         (Conv2D)\n",
            "bn4l_branch2c          (BatchNorm)\n",
            "res4m_branch2a         (Conv2D)\n",
            "bn4m_branch2a          (BatchNorm)\n",
            "res4m_branch2b         (Conv2D)\n",
            "bn4m_branch2b          (BatchNorm)\n",
            "res4m_branch2c         (Conv2D)\n",
            "bn4m_branch2c          (BatchNorm)\n",
            "res4n_branch2a         (Conv2D)\n",
            "bn4n_branch2a          (BatchNorm)\n",
            "res4n_branch2b         (Conv2D)\n",
            "bn4n_branch2b          (BatchNorm)\n",
            "res4n_branch2c         (Conv2D)\n",
            "bn4n_branch2c          (BatchNorm)\n",
            "res4o_branch2a         (Conv2D)\n",
            "bn4o_branch2a          (BatchNorm)\n",
            "res4o_branch2b         (Conv2D)\n",
            "bn4o_branch2b          (BatchNorm)\n",
            "res4o_branch2c         (Conv2D)\n",
            "bn4o_branch2c          (BatchNorm)\n",
            "res4p_branch2a         (Conv2D)\n",
            "bn4p_branch2a          (BatchNorm)\n",
            "res4p_branch2b         (Conv2D)\n",
            "bn4p_branch2b          (BatchNorm)\n",
            "res4p_branch2c         (Conv2D)\n",
            "bn4p_branch2c          (BatchNorm)\n",
            "res4q_branch2a         (Conv2D)\n",
            "bn4q_branch2a          (BatchNorm)\n",
            "res4q_branch2b         (Conv2D)\n",
            "bn4q_branch2b          (BatchNorm)\n",
            "res4q_branch2c         (Conv2D)\n",
            "bn4q_branch2c          (BatchNorm)\n",
            "res4r_branch2a         (Conv2D)\n",
            "bn4r_branch2a          (BatchNorm)\n",
            "res4r_branch2b         (Conv2D)\n",
            "bn4r_branch2b          (BatchNorm)\n",
            "res4r_branch2c         (Conv2D)\n",
            "bn4r_branch2c          (BatchNorm)\n",
            "res4s_branch2a         (Conv2D)\n",
            "bn4s_branch2a          (BatchNorm)\n",
            "res4s_branch2b         (Conv2D)\n",
            "bn4s_branch2b          (BatchNorm)\n",
            "res4s_branch2c         (Conv2D)\n",
            "bn4s_branch2c          (BatchNorm)\n",
            "res4t_branch2a         (Conv2D)\n",
            "bn4t_branch2a          (BatchNorm)\n",
            "res4t_branch2b         (Conv2D)\n",
            "bn4t_branch2b          (BatchNorm)\n",
            "res4t_branch2c         (Conv2D)\n",
            "bn4t_branch2c          (BatchNorm)\n",
            "res4u_branch2a         (Conv2D)\n",
            "bn4u_branch2a          (BatchNorm)\n",
            "res4u_branch2b         (Conv2D)\n",
            "bn4u_branch2b          (BatchNorm)\n",
            "res4u_branch2c         (Conv2D)\n",
            "bn4u_branch2c          (BatchNorm)\n",
            "res4v_branch2a         (Conv2D)\n",
            "bn4v_branch2a          (BatchNorm)\n",
            "res4v_branch2b         (Conv2D)\n",
            "bn4v_branch2b          (BatchNorm)\n",
            "res4v_branch2c         (Conv2D)\n",
            "bn4v_branch2c          (BatchNorm)\n",
            "res4w_branch2a         (Conv2D)\n",
            "bn4w_branch2a          (BatchNorm)\n",
            "res4w_branch2b         (Conv2D)\n",
            "bn4w_branch2b          (BatchNorm)\n",
            "res4w_branch2c         (Conv2D)\n",
            "bn4w_branch2c          (BatchNorm)\n",
            "res5a_branch2a         (Conv2D)\n",
            "bn5a_branch2a          (BatchNorm)\n",
            "res5a_branch2b         (Conv2D)\n",
            "bn5a_branch2b          (BatchNorm)\n",
            "res5a_branch2c         (Conv2D)\n",
            "res5a_branch1          (Conv2D)\n",
            "bn5a_branch2c          (BatchNorm)\n",
            "bn5a_branch1           (BatchNorm)\n",
            "res5b_branch2a         (Conv2D)\n",
            "bn5b_branch2a          (BatchNorm)\n",
            "res5b_branch2b         (Conv2D)\n",
            "bn5b_branch2b          (BatchNorm)\n",
            "res5b_branch2c         (Conv2D)\n",
            "bn5b_branch2c          (BatchNorm)\n",
            "res5c_branch2a         (Conv2D)\n",
            "bn5c_branch2a          (BatchNorm)\n",
            "res5c_branch2b         (Conv2D)\n",
            "bn5c_branch2b          (BatchNorm)\n",
            "res5c_branch2c         (Conv2D)\n",
            "bn5c_branch2c          (BatchNorm)\n",
            "fpn_c5p5               (Conv2D)\n",
            "fpn_c4p4               (Conv2D)\n",
            "fpn_c3p3               (Conv2D)\n",
            "fpn_c2p2               (Conv2D)\n",
            "fpn_p5                 (Conv2D)\n",
            "fpn_p2                 (Conv2D)\n",
            "fpn_p3                 (Conv2D)\n",
            "fpn_p4                 (Conv2D)\n",
            "In model:  rpn_model\n",
            "    rpn_conv_shared        (Conv2D)\n",
            "    rpn_class_raw          (Conv2D)\n",
            "    rpn_bbox_pred          (Conv2D)\n",
            "mrcnn_mask_conv1       (TimeDistributed)\n",
            "mrcnn_mask_bn1         (TimeDistributed)\n",
            "mrcnn_mask_conv2       (TimeDistributed)\n",
            "mrcnn_mask_bn2         (TimeDistributed)\n",
            "mrcnn_class_conv1      (TimeDistributed)\n",
            "mrcnn_class_bn1        (TimeDistributed)\n",
            "mrcnn_mask_conv3       (TimeDistributed)\n",
            "mrcnn_mask_bn3         (TimeDistributed)\n",
            "mrcnn_class_conv2      (TimeDistributed)\n",
            "mrcnn_class_bn2        (TimeDistributed)\n",
            "mrcnn_mask_conv4       (TimeDistributed)\n",
            "mrcnn_mask_bn4         (TimeDistributed)\n",
            "mrcnn_bbox_fc          (TimeDistributed)\n",
            "mrcnn_mask_deconv      (TimeDistributed)\n",
            "mrcnn_class_logits     (TimeDistributed)\n",
            "mrcnn_mask             (TimeDistributed)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mget_attr\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mc_api_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2380\u001b[0;31m         \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_OperationGetAttrValueProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2381\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Operation 'truediv_427' has no attr named '_XlaCompile'.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m_MaybeCompile\u001b[0;34m(scope, op, func, grad_fn)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m       \u001b[0mxla_compile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_XlaCompile\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m       xla_separate_compiled_gradients = op.get_attr(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mget_attr\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2383\u001b[0m       \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2384\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2385\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Operation 'truediv_427' has no attr named '_XlaCompile'.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-dfae757cb639>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m             \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m             layers=\"all\")\n\u001b[0m",
            "\u001b[0;32m/content/Mask_RCNN/mrcnn/model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_dataset, val_dataset, learning_rate, epochs, layers, augmentation, custom_callbacks, no_augmentation_sources)\u001b[0m\n\u001b[1;32m   2372\u001b[0m             \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2373\u001b[0m             \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2374\u001b[0;31m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2375\u001b[0m         )\n\u001b[1;32m   2376\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdo_validation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_test_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_make_train_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    510\u001b[0m                     training_updates = self.optimizer.get_updates(\n\u001b[1;32m    511\u001b[0m                         \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_collected_trainable_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m                         loss=self.total_loss)\n\u001b[0m\u001b[1;32m    513\u001b[0m                 updates = (self.updates +\n\u001b[1;32m    514\u001b[0m                            \u001b[0mtraining_updates\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/optimizers.py\u001b[0m in \u001b[0;36mget_updates\u001b[0;34m(self, loss, params)\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_updates_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_updates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/optimizers.py\u001b[0m in \u001b[0;36mget_gradients\u001b[0;34m(self, loss, params)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             raise ValueError('An operation has `None` for gradient. '\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mgradients\u001b[0;34m(loss, variables)\u001b[0m\n\u001b[1;32m   3021\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mgradients\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3022\u001b[0m     \"\"\"\n\u001b[0;32m-> 3023\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3024\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gradients_impl.py\u001b[0m in \u001b[0;36mgradients\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_ys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mgate_gradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maggregation_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         unconnected_gradients)\n\u001b[0m\u001b[1;32m    159\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m_GradientsHelper\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\u001b[0m\n\u001b[1;32m    677\u001b[0m                 \u001b[0;31m# functions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m                 in_grads = _MaybeCompile(grad_scope, op, func_call,\n\u001b[0;32m--> 679\u001b[0;31m                                          lambda: grad_fn(op, *out_grads))\n\u001b[0m\u001b[1;32m    680\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m                 \u001b[0;31m# For function call ops, we add a 'SymbolicGradient'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m_MaybeCompile\u001b[0;34m(scope, op, func, grad_fn)\u001b[0m\n\u001b[1;32m    348\u001b[0m       \u001b[0mxla_scope\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_XlaScope\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Exit early\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mxla_compile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    677\u001b[0m                 \u001b[0;31m# functions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m                 in_grads = _MaybeCompile(grad_scope, op, func_call,\n\u001b[0;32m--> 679\u001b[0;31m                                          lambda: grad_fn(op, *out_grads))\n\u001b[0m\u001b[1;32m    680\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m                 \u001b[0;31m# For function call ops, we add a 'SymbolicGradient'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py\u001b[0m in \u001b[0;36m_RealDivGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m   1275\u001b[0m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1277\u001b[0;31m   \u001b[0msx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1278\u001b[0m   \u001b[0msy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m   \u001b[0mrx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_gradient_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/array_ops.py\u001b[0m in \u001b[0;36mshape\u001b[0;34m(input, name, out_type)\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mout_type\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m   \"\"\"\n\u001b[0;32m--> 447\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mshape_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/array_ops.py\u001b[0m in \u001b[0;36mshape_internal\u001b[0;34m(input, name, optimize, out_type)\u001b[0m\n\u001b[1;32m    464\u001b[0m   \"\"\"\n\u001b[1;32m    465\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Shape\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m     if isinstance(\n\u001b[0m\u001b[1;32m    467\u001b[0m         input, (sparse_tensor.SparseTensor, sparse_tensor.SparseTensorValue)):\n\u001b[1;32m    468\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24Vufpw-yiML",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save weights\n",
        "# Typically not needed because callbacks save after every epoch\n",
        "# Uncomment to save manually\n",
        "# model_path = os.path.join(MODEL_DIR, \"mask_rcnn_shapes.h5\")\n",
        "# model.keras_model.save_weights(model_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zm38ph2ayiMM",
        "colab_type": "text"
      },
      "source": [
        "## Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YkVM7yPyiMN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "05fd2539-9b1a-4b1c-9d8f-9b30d8f51dcd"
      },
      "source": [
        "class InferenceConfig(ShapesConfig):\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        "\n",
        "inference_config = InferenceConfig()\n",
        "\n",
        "# Recreate the model in inference mode\n",
        "model = modellib.MaskRCNN(mode=\"inference\", \n",
        "                          config=inference_config,\n",
        "                          model_dir=MODEL_DIR)\n",
        "\n",
        "# Get path to saved weights\n",
        "# Either set a specific path or find last trained weights\n",
        "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
        "model_path = model.find_last()\n",
        "\n",
        "# Load trained weights\n",
        "print(\"Loading weights from \", model_path)\n",
        "model.load_weights(model_path, by_name=True)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/Mask_RCNN/mrcnn/model.py:720: The name tf.sets.set_intersection is deprecated. Please use tf.sets.intersection instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/Mask_RCNN/mrcnn/model.py:722: The name tf.sparse_tensor_to_dense is deprecated. Please use tf.sparse.to_dense instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/Mask_RCNN/mrcnn/model.py:772: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "Loading weights from  /logs/shapes20191222T1342/mask_rcnn_shapes_0001.h5\n",
            "Re-starting from epoch 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDyVaeMFyiMP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "outputId": "9e1e10ed-5e41-4f77-b20d-7ade2027e7e2"
      },
      "source": [
        "# Test on a random image\n",
        "image_id = random.choice(dataset_val.image_ids)\n",
        "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
        "    modellib.load_image_gt(dataset_val, inference_config, \n",
        "                           image_id, use_mini_mask=False)\n",
        "\n",
        "log(\"original_image\", original_image)\n",
        "log(\"image_meta\", image_meta)\n",
        "log(\"gt_class_id\", gt_class_id)\n",
        "log(\"gt_bbox\", gt_bbox)\n",
        "log(\"gt_mask\", gt_mask)\n",
        "\n",
        "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
        "                            dataset_train.class_names, figsize=(8, 8))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original_image           shape: (128, 128, 3)         min:   10.00000  max:  233.00000  uint8\n",
            "image_meta               shape: (16,)                 min:    0.00000  max:  128.00000  int64\n",
            "gt_class_id              shape: (1,)                  min:    2.00000  max:    2.00000  int32\n",
            "gt_bbox                  shape: (1, 4)                min:   44.00000  max:  127.00000  int32\n",
            "gt_mask                  shape: (128, 128, 1)         min:    0.00000  max:    1.00000  bool\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcEAAAHBCAYAAAARuwDoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAGAWEQQAmEUEAQBm/T8s0PUYWbz/WQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJuc8DYZyiMQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "outputId": "05ebbb74-1a6e-42cb-8a82-76cd1aacaf0f"
      },
      "source": [
        "results = model.detect([original_image], verbose=1)\n",
        "\n",
        "r = results[0]\n",
        "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
        "                            dataset_val.class_names, r['scores'], ax=get_ax())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing 1 images\n",
            "image                    shape: (128, 128, 3)         min:   10.00000  max:  233.00000  uint8\n",
            "molded_images            shape: (1, 128, 128, 3)      min:  -93.90000  max:  116.20000  float64\n",
            "image_metas              shape: (1, 16)               min:    0.00000  max:  128.00000  int64\n",
            "anchors                  shape: (1, 4092, 4)          min:   -0.71267  max:    1.20874  float32\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcEAAAHBCAYAAAARuwDoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwi+wAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_C4MMuZyiMS",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ws8z2dgVyiMS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b662f0fb-0c8a-4599-e2a5-223f3df833e6"
      },
      "source": [
        "# Compute VOC-Style mAP @ IoU=0.5\n",
        "# Running on 10 images. Increase for better accuracy.\n",
        "image_ids = np.random.choice(dataset_val.image_ids, 10)\n",
        "APs = []\n",
        "for image_id in image_ids:\n",
        "    # Load image and ground truth data\n",
        "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
        "        modellib.load_image_gt(dataset_val, inference_config,\n",
        "                               image_id, use_mini_mask=False)\n",
        "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
        "    # Run object detection\n",
        "    results = model.detect([image], verbose=0)\n",
        "    r = results[0]\n",
        "    # Compute AP\n",
        "    AP, precisions, recalls, overlaps =\\\n",
        "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
        "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
        "    APs.append(AP)\n",
        "    \n",
        "print(\"mAP: \", np.mean(APs))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mAP:  0.9222222258647281\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1tbhZ76yiMU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}